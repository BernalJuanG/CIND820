---
title: "Testing Clustering"
author: "Juan Bernal"
date: "21/06/2021"
output: html_document
---
<h1>Trying some preliminary k-means clustering - with one-hot encoding</h1>


<h3>Libraries</h3>
```{r,echo=FALSE}
library(tidyverse)
library(factoextra) #For visualizing the clusters
library(cluster) #For silhouette metric
library(clValid)
```

<h3>Load dataframe from other file</h3>
```{r}
#This loads the files I had saved in the NY Airbnb EDA Draft 2.rmd file with the save() function
load(file="airbnb_df_encoded.Rdata") 
load(file="airbnb_df.Rdata")

```

<h3>Scale the data</h3>
```{r}
#Scaling only the numerical variables. The one-hot encoded variables don't need to be scaled.
airbnb_df_scaled<-airbnb_df_encoded
airbnb_df_scaled[,c(7:14)]<-scale(airbnb_df_scaled[,c(7:14)])
```

<h3>Determine K</h3>
```{r}
#This will use the elbow method to determine which k value reduces the within variability of the clusters the most. The outcome is about 6.


#This function will run kmeans() k times and return the total WSS of the clusters.
wss <- function(k) {
  kmeans(airbnb_df_scaled, k, nstart = 10 )$tot.withinss #The kproto object has the total Within Sum of Squares of the clusters. We want a K value that reduces this.
}
k.values <- 1:15

wss_values <- map_dbl(k.values, wss)#Applies the wss function to each element in k.values


plot(k.values, wss_values,
       type="b", pch = 19, frame = FALSE,
       main="Reduction in WSS per K value",
       xlab="Number of clusters K",
       ylab="Total within-clusters sum of squares")

```

<h3>K-Means Clustering on all the variables</h3>
```{r}
set.seed(123)
kmeans_basic<-kmeans(airbnb_df_scaled,centers=6)
```


<h3>Attach the cluster as labels on the original dataset</h3>
```{r}
airbnb_df_clustered<-airbnb_df %>%
  mutate("cluster"=kmeans_basic$cluster,.before=neighbourhood_group)
```

<h3>Plot data faceted by cluster</h3>
```{r}


neighbourhoodGroupByCluster<-ggplot(data=airbnb_df_clustered,aes(x=neighbourhood_group))+
         geom_bar()+
         labs(title="Neighbourhood Groups distribution in each cluster",x="neighbourhood_group",y="Number of listings")+
         coord_flip()+
         facet_wrap(~cluster,nrow=3)

neighbourhoodGroupByCluster
```


```{r}
priceByCluster<-ggplot(data=airbnb_df_clustered,aes(x=price))+
         geom_histogram()+
         labs(title="Price distribution in each cluster",x="price",y="Number of listings")+
         facet_wrap(~cluster,nrow=3)

priceByCluster
```

```{r}
room_typeByCluster<-ggplot(data=airbnb_df_clustered,aes(x=room_type))+
         geom_bar()+
         labs(title="Room type distribution in each cluster",x="room_type",y="Number of listings")+
         facet_wrap(~cluster,nrow=3)
room_typeByCluster
```
```{r}
ggplot(airbnb_df_clustered,aes(x=price,y=reviews_per_month,colour=factor(cluster)))+
  geom_point()+
  facet_wrap(~cluster,nrow=3)


```

<h3>Plot clusters on 2D Map</h3>
```{r}
fviz_cluster(kmeans_basic, data = airbnb_df_encoded, geom = c("point"),ellipse.type = "euclid")
```

<h3>Evaluating the clusters with Dunn Index metric</h3>

```{r}
#On my machine the dunn() and connectivity() algorithms could not run without allocating more memory with the memory.limit() function.
memory.size() ### Checking your memory size
memory.limit(size=56000) ## Checking the set limit 
```

```{r}
#Commenting this out because it takes a very long time to run. I ran it just once.
#clValid::dunn(Data=airbnb_df_clustered,clusters=kmeans_basic$cluster) #0.001989926
```
<h3>Evaluating the clusters with Connectivity metric</h3>

```{r}
clValid::connectivity(Data=airbnb_df_clustered,clusters=kmeans_basic$cluster) #28123.34
```
